{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'gap-coreference', 'gendered-pronoun-resolution']\n",
      "CPU times: user 1.69 ms, sys: 189 µs, total: 1.88 ms\n",
      "Wall time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../data\"))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '../data/'\n",
    "GAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap-coreference')\n",
    "SUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\n",
    "FAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 ms, sys: 12.3 ms, total: 54.6 ms\n",
      "Wall time: 54.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\n",
    "train_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\n",
    "dev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\n",
    "\n",
    "train_df = pd.read_csv(train_df_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_df_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_df_path, sep='\\t')\n",
    "\n",
    "#pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 499 µs, sys: 56 µs, total: 555 µs\n",
      "Wall time: 533 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text Pronoun  \\\n",
       "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2  test-3  Though his emigration from the country has aff...      He   \n",
       "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
       "4  test-5  It is about a pair of United States Navy shore...     his   \n",
       "\n",
       "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
       "0             383     Bob Suter       352    False              Dehner   \n",
       "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
       "2             312  Ali Aladhadh       256     True              Saddam   \n",
       "3             526       Alliata       377    False           Pisciotta   \n",
       "4             406         Eddie       421     True         Rock Reilly   \n",
       "\n",
       "   B-offset  B-coref                                             URL  \n",
       "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
       "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
       "2       295    False           http://en.wikipedia.org/wiki/Aladhadh  \n",
       "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
       "4       559    False            http://en.wikipedia.org/wiki/Chasers  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import DependencyParser\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def _get_preceding_words(tokens, offset, k):\n",
    "    start = offset - k\n",
    "    \n",
    "    precedings = [None] * max(0, 0-start)\n",
    "    start = max(0, start)\n",
    "    precedings += tokens[start: offset]\n",
    "    \n",
    "    return precedings\n",
    "\n",
    "def _get_following_words(tokens, offset, k):\n",
    "    end = offset + k\n",
    "    \n",
    "    followings = [None] * max(0, end - len(tokens))\n",
    "    end = min(len(tokens), end)\n",
    "    followings += tokens[offset: end]\n",
    "    \n",
    "    return followings\n",
    "        \n",
    "\n",
    "def extrac_embed_features_tokens(text, char_offset):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset = bs(lens, char_offset) - 1\n",
    "    # mention_word\n",
    "    mention = doc[mention_offset]\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index = bs(acc_lens, mention_offset)\n",
    "    # mention sentence\n",
    "    sent = list(doc.sents)[sent_index]\n",
    "    \n",
    "    # dependency parent\n",
    "    head = mention.head\n",
    "    \n",
    "    # last word and first word\n",
    "    first_word, last_word = sent[0], sent[-2]\n",
    "    \n",
    "    assert mention_offset >= 0\n",
    "    \n",
    "    # two preceding words and two following words\n",
    "    tokens = list(doc)\n",
    "    precedings2 = _get_preceding_words(tokens, mention_offset, 2)\n",
    "    followings2 = _get_following_words(tokens, mention_offset, 2)\n",
    "    \n",
    "    # five preceding words and five following words\n",
    "    precedings5 = _get_preceding_words(tokens, mention_offset, 5)\n",
    "    followings5 = _get_following_words(tokens, mention_offset, 5)\n",
    "    \n",
    "    # sentence words\n",
    "    sent_tokens = [token for token in sent]\n",
    "    \n",
    "    return mention, head, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: \n",
      "Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\n",
      "\n",
      "Dependency parsing trees: \n",
      "                    played                                           \n",
      "  ____________________|_______________________                        \n",
      " |         |                              girlfriend                 \n",
      " |         |                   _______________|____________           \n",
      " |         |                  |                            of        \n",
      " |         |                  |                            |          \n",
      " |      Telford            officer                       Simon       \n",
      " |    _____|_____      _______|______                  ____|_____     \n",
      " .  Zoe          --  the           police             ,        Maggie\n",
      "\n",
      "                                 Dumped                                   \n",
      "  _________________________________|______________                         \n",
      " |   |    |          in                           |                       \n",
      " |   |    |          |                            |                        \n",
      " |   |    |       episode                         |                       \n",
      " |   |    |     _____|______                      |                        \n",
      " |   |    |    |     |      of                  slept                     \n",
      " |   |    |    |     |      |       ______________|______________          \n",
      " |   |    by   |     |    series   |     |   |    |    with     seen      \n",
      " |   |    |    |     |      |      |     |   |    |     |     ___|_____    \n",
      " ,   .  Simon the  final    1    after   he  ,   and  Jenny  is not  again\n",
      "\n",
      "                      played                              \n",
      "  ______________________|_______                           \n",
      " |    |                      Cassidy                      \n",
      " |    |       __________________|___________               \n",
      " |    |      |     |    |       |         pupil           \n",
      " |    |      |     |    |       |      _____|__________    \n",
      " |    |      |     |    |       |     |     |    |     in \n",
      " |    |      |     |    |       |     |     |    |     |   \n",
      " |    |      |     |    |     friend  |     |    |   class\n",
      " |    |      |     |    |       |     |     |    |     |   \n",
      " |  Thomas   |     |    |    Pauline  |     |   year Simon\n",
      " |    |      |     |    |       |     |     |    |     |   \n",
      " .  Phoebe Cheryl  ,   and      's   also   a    11    's \n",
      "\n",
      "                                            Dumped                                                               \n",
      "  ____________________________________________|_________                                                          \n",
      " |      |         |                                    have                                                      \n",
      " |      |         |        _____________________________|_________________                                        \n",
      " |      |         |       |    |    |    |    |     |   |              realised                                  \n",
      " |      |         |       |    |    |    |    |     |   |      ___________|_______                                \n",
      " |      |         |       |    |    |    |    |     |   |     |                  was                             \n",
      " |      |         |       |    |    |    |    |     |   |     |     ______________|____                           \n",
      " |      |         |       |    |    |    |    |     |   |     |    |                  due                        \n",
      " |      |         |       |    |    |    |    |     |   |     |    |       ____________|_____                     \n",
      " |      |     following   |    |    |    |    |     |   |     |    |      |               catching               \n",
      " |      |         |       |    |    |    |    |     |   |     |    |      |        __________|_______             \n",
      " |      |       advice    |    |    |    |    |     |   |     |    |      |       |                 off          \n",
      " |      |         |       |    |    |    |    |     |   |     |    |      |       |                  |            \n",
      " |  boyfriend   Simon     |    |    |    |    |     |  with   |    |      to      |                friend        \n",
      " |      |         |       |    |    |    |    |     |   |     |    |      |       |           _______|_______     \n",
      " .     her        's    after  he would n't  sex   but her  later this   him    crabs       her           Pauline\n",
      "\n",
      "\n",
      "Features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mention                                                      her\n",
       "parent                                                 boyfriend\n",
       "first_word                                                Dumped\n",
       "last_word                                                Pauline\n",
       "precedings2                                          [., Dumped]\n",
       "followings2                                     [her, boyfriend]\n",
       "precedings5                        [Simon, 's, class, ., Dumped]\n",
       "followings5               [her, boyfriend, following, Simon, 's]\n",
       "sent_tokens    [Dumped, her, boyfriend, following, Simon, 's,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Texts: \")\n",
    "text = u\"Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\"\n",
    "print(text)\n",
    "\n",
    "print(\"\\nDependency parsing trees: \")\n",
    "doc = nlp(text)\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "print(\"\\nFeatures:\")\n",
    "mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text, 274)\n",
    "features = pd.Series([str(feature) for feature in (mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens)], index=['mention', 'parent', 'first_word', 'last_word', 'precedings2', 'followings2', 'precedings5', 'followings5', 'sent_tokens'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embed_features = 11\n",
    "embed_dim = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_features(df, text_column, offset_column):\n",
    "    text_offset_list = df[[text_column, offset_column]].values.tolist()\n",
    "    num_features = num_embed_features\n",
    "    \n",
    "    embed_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features, embed_dim))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text_offset[0], text_offset[1])\n",
    "        \n",
    "        feature_index = 0\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = mention.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = parent.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = first_word.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = last_word.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings2])\n",
    "        feature_index += len(precedings2)\n",
    "        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings2])\n",
    "        feature_index += len(followings2)\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings5]), axis=0)\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings5]), axis=0)\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector for token in sent_tokens]), axis=0) if len(sent_tokens) > 0 else np.zeros(embed_dim)\n",
    "        feature_index += 1\n",
    "    \n",
    "    return embed_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo\n",
    "\n",
    "def ohe_dist(dist, buckets):\n",
    "    idx = bs_(buckets, dist)\n",
    "    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n",
    "    oh[idx] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrac_positional_features(text, char_offset1, char_offset2):\n",
    "    doc = nlp(text)\n",
    "    max_len = 64\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset1 = bs(lens, char_offset1) - 1\n",
    "    mention_offset2 = bs(lens, char_offset2) - 1\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index1 = bs(acc_lens, mention_offset1)\n",
    "    sent_index2 = bs(acc_lens, mention_offset2)\n",
    "    \n",
    "    sent1 = list(doc.sents)[sent_index1]\n",
    "    sent2 = list(doc.sents)[sent_index2]\n",
    "    \n",
    "    # buckets\n",
    "    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n",
    "    \n",
    "    # relative distance\n",
    "    dist = mention_offset2 - mention_offset1\n",
    "    dist_oh = ohe_dist(dist, bucket_dist)\n",
    "    \n",
    "    # buckets\n",
    "    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n",
    "    \n",
    "    # absolute position in the sentence\n",
    "    sent_pos1 = mention_offset1 + 1\n",
    "    if sent_index1 > 0:\n",
    "        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n",
    "    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n",
    "    sent_pos_inv1 = len(sent1) - sent_pos1\n",
    "    assert sent_pos_inv1 >= 0\n",
    "    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n",
    "    \n",
    "    sent_pos2 = mention_offset2 + 1\n",
    "    if sent_index2 > 0:\n",
    "        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n",
    "    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n",
    "    sent_pos_inv2 = len(sent2) - sent_pos2\n",
    "    if sent_pos_inv2 < 0:\n",
    "        print(sent_pos_inv2)\n",
    "        print(len(sent2))\n",
    "        print(sent_pos2)\n",
    "        raise ValueError\n",
    "    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n",
    "    \n",
    "    sent_pos_ratio1 = sent_pos1 / len(sent1)\n",
    "    sent_pos_ratio2 = sent_pos2 / len(sent2)\n",
    "    \n",
    "    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_features = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    num_features = num_pos_features\n",
    "    \n",
    "    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n",
    "        \n",
    "        feature_index = 0\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n",
    "        feature_index += len(dist_oh)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n",
    "        feature_index += len(sent_pos_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n",
    "        feature_index += len(sent_pos_oh2)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n",
    "        feature_index += len(sent_pos_inv_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n",
    "        feature_index += len(sent_pos_inv_oh2)\n",
    "    \n",
    "    return pos_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait ...\n",
      "(2000, 11, 384)\n",
      "(454, 11, 384)\n",
      "(2000, 11, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"wait ...\")\n",
    "p_emb_tra = create_embedding_features(train_df, 'Text', 'Pronoun-offset')\n",
    "print(p_emb_tra.shape)\n",
    "p_emb_dev = create_embedding_features(dev_df, 'Text', 'Pronoun-offset')\n",
    "print(p_emb_dev.shape)\n",
    "p_emb_test = create_embedding_features(test_df, 'Text', 'Pronoun-offset')\n",
    "print(p_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11, 384)\n",
      "(454, 11, 384)\n",
      "(2000, 11, 384)\n"
     ]
    }
   ],
   "source": [
    "a_emb_tra = create_embedding_features(train_df, 'Text', 'A-offset')\n",
    "a_emb_dev = create_embedding_features(dev_df, 'Text', 'A-offset')\n",
    "a_emb_test = create_embedding_features(test_df, 'Text', 'A-offset')\n",
    "print(a_emb_tra.shape)\n",
    "print(a_emb_dev.shape)\n",
    "print(a_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11, 384)\n",
      "(454, 11, 384)\n",
      "(2000, 11, 384)\n"
     ]
    }
   ],
   "source": [
    "b_emb_tra = create_embedding_features(train_df, 'Text', 'B-offset')\n",
    "b_emb_dev = create_embedding_features(dev_df, 'Text', 'B-offset')\n",
    "b_emb_test = create_embedding_features(test_df, 'Text', 'B-offset')\n",
    "\n",
    "print(b_emb_tra.shape)\n",
    "print(b_emb_dev.shape)\n",
    "print(b_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 45)\n",
      "(454, 45)\n",
      "(2000, 45)\n"
     ]
    }
   ],
   "source": [
    "pa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "\n",
    "print(pa_pos_tra.shape)\n",
    "print(pa_pos_dev.shape)\n",
    "print(pa_pos_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 45)\n",
      "(454, 45)\n",
      "(2000, 45)\n"
     ]
    }
   ],
   "source": [
    "pb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "print(pb_pos_tra.shape)\n",
    "print(pb_pos_dev.shape)\n",
    "print(pb_pos_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(454,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = dev_df.apply(_row_to_y, axis=1)\n",
    "y_test = test_df.apply(_row_to_y, axis=1)\n",
    "print(y_tra.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "X_train = [p_emb_tra, a_emb_tra, b_emb_tra, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [p_emb_dev, a_emb_dev, b_emb_dev, pa_pos_dev, pb_pos_dev]\n",
    "X_test = [p_emb_test, a_emb_test, b_emb_test, pa_pos_test, pb_pos_test]\n",
    "print(len(X_train))\n",
    "print(len(X_dev))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    model_dim, mlp_dim, mlp_depth=1, drop_out=0.5, return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, feature_map_layer, flatten_layer, x):\n",
    "        x = feature_dropout_layer(x)\n",
    "        x = feature_map_layer(x)\n",
    "        x = flatten_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer1 = layers.TimeDistributed(layers.Dense(model_dim, name=\"feature_map_layer1\", activation=\"relu\"))\n",
    "    flatten_layer1 = layers.Flatten(name=\"feature_flatten_layer1\")\n",
    "    feature_map_layer2 = layers.Dense(model_dim, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, feature_map_layer1, flatten_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(x1+x2)\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_channel_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param num_filters: list of integers\n",
    "        The number of filters.\n",
    "    :param filter_sizes: list of integers\n",
    "        The kernel size.\n",
    "    :param pooling: str, either 'max' or 'average'\n",
    "        Pooling method.\n",
    "    :param padding: One of \"valid\", \"causal\" or \"same\" (case-insensitive).\n",
    "        Padding method.\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, cnns, pools, concate_layer1, x):\n",
    "        x = feature_dropout_layer(x)\n",
    "        pooled_outputs = []\n",
    "        for i in range(len(cnns)):\n",
    "            conv = cnns[i](x)\n",
    "            if pooling == 'max':\n",
    "                conv = pools[i](conv)\n",
    "            else:\n",
    "                conv = pools[i](conv)\n",
    "            pooled_outputs.append(conv)\n",
    "        \n",
    "        if len(cnns) == 1:\n",
    "            x = conv\n",
    "        else:\n",
    "            x = concate_layer1(pooled_outputs)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # CNN Layers\n",
    "    cnns = []\n",
    "    pools = []\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    for i in range(len(filter_sizes)):\n",
    "        cnns.append(layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu', name=\"cc_layer1\" + str(i)))\n",
    "        if pooling == 'max':\n",
    "            pools.append(layers.GlobalMaxPooling1D(name='global_pooling_layer1' + str(i)))\n",
    "        else:\n",
    "            pools.append(layers.GlobalAveragePooling1D(name='global_pooling_layer1' + str(i)))\n",
    "    concate_layer1 = layers.Concatenate(name='concated_layer')\n",
    "    \n",
    "    feature_map_layer2 = layers.Dense(model_dim, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, cnns, pools, concate_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(x1+x2)\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class RemappedCoAttentionWeight(merge._Merge):\n",
    "    \"\"\"\n",
    "        Unnormalized Co-Attention operation for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Ankur et al. [https://aclweb.org/anthology/D16-1244]\n",
    "        \"A Decomposable Attention Model for Natural Language Inference\"\n",
    "        # Input shape\n",
    "            List of 2 3D tensor with shape: `(samples, steps1, features1)` and `(samples, steps2, features2)`.\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(samples, steps1, step2)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, activation='sigmoid',\n",
    "                 W1_regularizer=None,  b1_regularizer=None,\n",
    "                 W1_constraint=None, b1_constraint=None,\n",
    "                 bias1=True, **kwargs):\n",
    "\n",
    "        self.model_size = model_size\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W1_regularizer = regularizers.get(W1_regularizer)\n",
    "        self.b1_regularizer = regularizers.get(b1_regularizer)\n",
    "\n",
    "        self.W1_constraint = constraints.get(W1_constraint)\n",
    "        self.b1_constraint = constraints.get(b1_constraint)\n",
    "\n",
    "        self.bias1 = bias1\n",
    "        self.activation = activations.get(activation)\n",
    "        super(RemappedCoAttentionWeight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(RemappedCoAttentionWeight, self).build(input_shape)\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError(\"input must be a size two list which contains two tensors\")\n",
    "\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        self.W1 = self.add_weight((self.model_size, shape1[-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W1'.format(self.name),\n",
    "                                 regularizer=self.W1_regularizer,\n",
    "                                 constraint=self.W1_constraint)\n",
    "\n",
    "        self.W2 = self.W1\n",
    "\n",
    "        if self.bias1:\n",
    "            self.b1 = self.add_weight((self.model_size,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b1'.format(self.name),\n",
    "                                     regularizer=self.b1_regularizer,\n",
    "                                     constraint=self.b1_constraint)\n",
    "\n",
    "        if self.bias1:\n",
    "            self.b2 = self.b1\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # pass the mask to the next layers\n",
    "        return input_mask\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        if len(inputs) != 2:\n",
    "            raise ValueError('A `Subtract` layer should be called '\n",
    "                             'on exactly 2 inputs')\n",
    "\n",
    "        x1, x2 = inputs[0], inputs[1]\n",
    "\n",
    "        # u = Wx + b\n",
    "        u1 = _dot_product(x1, self.W1)\n",
    "        if self.bias1:\n",
    "            u1 += self.b1\n",
    "\n",
    "        u2 = _dot_product(x2, self.W2)\n",
    "        if self.bias1:\n",
    "            u2 += self.b2\n",
    "\n",
    "        # u = Activation(Wx + b)\n",
    "        u1 = self.activation(u1)\n",
    "        u2 = self.activation(u2)\n",
    "\n",
    "        # atten = exp(u1 u2^T)\n",
    "        atten = K.batch_dot(u1, u2, axes=[2, 2])\n",
    "        atten = K.exp(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Dot` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        if shape1[0] != shape2[0]:\n",
    "            raise ValueError(\"batch size must be same\")\n",
    "\n",
    "        return shape1[0], shape1[1], shape2[1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'activation': self.activation,\n",
    "            'model_size': self.model_size,\n",
    "            'W1_regularizer': regularizers.serialize(self.W1_regularizer),\n",
    "            'b1_regularizer': regularizers.serialize(self.b1_regularizer),\n",
    "            'W1_constraint': constraints.serialize(self.W1_constraint),\n",
    "            'b1_constraint': constraints.serialize(self.b1_constraint),\n",
    "            'bias1': self.bias1,\n",
    "        }\n",
    "        base_config = super(RemappedCoAttentionWeight, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class FeatureNormalization(Layer):\n",
    "    \"\"\"\n",
    "        Normalize feature along a specific axis.\n",
    "        Supports Masking.\n",
    "\n",
    "        # Input shape\n",
    "            A ND tensor with shape: `(samples, feature1 ... featuresN).\n",
    "        # Output shape\n",
    "            ND tensor with shape: `(samples, feature1 ... featuresN)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "\n",
    "        self.axis = axis\n",
    "        self.supports_masking = True\n",
    "        super(FeatureNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FeatureNormalization, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # don't pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a = K.cast(mask, K.floatx()) * inputs\n",
    "        else:\n",
    "            a = inputs\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=self.axis, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis\n",
    "        }\n",
    "        base_config = super(FeatureNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inter_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model with Coattention Mechanism.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, x):\n",
    "        #x = feature_dropout_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer2 = layers.Dense(feature_dim1, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    # From mention-pair embeddings\n",
    "    reshape_layer = layers.Reshape((1, feature_dim1), name=\"reshape_layer\")\n",
    "    x2 = [reshape_layer(x2_) for x2_ in x2]\n",
    "    pair1 = layers.Concatenate(axis=1, name=\"concate_pair1_layer\")([x1[0], x1[1], x2[0]])\n",
    "    pair2 = layers.Concatenate(axis=1, name=\"concate_pair2_layer\")([x1[0], x1[2], x2[1]])\n",
    "    \n",
    "    coatten_layer = RemappedCoAttentionWeight(atten_dim, name=\"coattention_weights_layer\")\n",
    "    featnorm_layer1 = FeatureNormalization(name=\"normalized_coattention_weights_layer1\", axis=1)\n",
    "    featnorm_layer2 = FeatureNormalization(name=\"normalized_coattention_weights_layer2\", axis=2)\n",
    "    focus_layer1 = layers.Dot((1, 1), name=\"focus_layer1\")\n",
    "    focus_layer2 = layers.Dot((2, 1), name=\"focus_layer2\")\n",
    "    pair_layer1 = layers.Concatenate(axis=-1, name=\"pair_layer1\")\n",
    "    pair_layer2 = layers.Concatenate(axis=-1, name=\"pair_layer2\")\n",
    "    \n",
    "    # attention\n",
    "    attens = coatten_layer([pair1, pair2])\n",
    "    attens1 = featnorm_layer1(attens)\n",
    "    attens2 = featnorm_layer2(attens)\n",
    "    # compare\n",
    "    focus1 = focus_layer1([attens1, pair1])\n",
    "    focus2 = focus_layer2([attens2, pair2])\n",
    "    pair1 = pair_layer1([pair1, focus2])\n",
    "    pair2 = pair_layer2([pair2, focus1])\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")([pair1, pair2])\n",
    "    x = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"pair_dropout_layer\"))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(mlp_dim, name=\"pair_feature_map_layer\", activation=\"relu\"))(x)\n",
    "    x = layers.Flatten(name=\"pair_feature_flatten_layer1\")(x)\n",
    "    \n",
    "#     pooled_outputs = []\n",
    "#     for i in range(len(filter_sizes)):\n",
    "#         conv = layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu')(x)\n",
    "#         if pooling == 'max':\n",
    "#             conv = layers.GlobalMaxPooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         else:\n",
    "#             conv = layers.GlobalAveragePooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         pooled_outputs.append(conv)\n",
    "#     if len(pooled_outputs) > 1:\n",
    "#         x = layers.Concatenate(name='concated_layer')(pooled_outputs)\n",
    "#     else:\n",
    "#         x = conv\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'RemappedCoAttentionWeight': RemappedCoAttentionWeight,\n",
    "                       \"FeatureNormalization\": FeatureNormalization}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intra_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model with Coattention Mechanism.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, x):\n",
    "        #x = feature_dropout_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def coatten_compare(\n",
    "        feature_concat_layer, coatten_layer, \n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        mention, entity, mention_entity_feature):\n",
    "        \n",
    "        x1 = feature_concat_layer([entity, mention_entity_feature])\n",
    "        x2 = feature_concat_layer([mention, mention_entity_feature])\n",
    "        \n",
    "        # attention\n",
    "        attens = coatten_layer([x1, x2])\n",
    "        attens1 = featnorm_layer1(attens)\n",
    "        attens2 = featnorm_layer2(attens)\n",
    "        # compare\n",
    "        focus1 = focus_layer1([attens1, x1])\n",
    "        focus2 = focus_layer2([attens2, x2])\n",
    "        x1 = pair_layer1([x1, focus2])\n",
    "        x2 = pair_layer2([x2, focus1])\n",
    "        \n",
    "        return x1, x2\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer2 = layers.Dense(feature_dim1, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    # From mention-pair embeddings\n",
    "    reshape_layer = layers.Reshape((1, feature_dim1), name=\"reshape_layer\")\n",
    "    x2 = [reshape_layer(x2_) for x2_ in x2]\n",
    "    \n",
    "    feature_concat_layer = layers.Concatenate(axis=1, name=\"concate_pair_layer\")\n",
    "    coatten_layer = RemappedCoAttentionWeight(atten_dim, name=\"coattention_weights_layer\")\n",
    "    featnorm_layer1 = FeatureNormalization(name=\"normalized_coattention_weights_layer1\", axis=1)\n",
    "    featnorm_layer2 = FeatureNormalization(name=\"normalized_coattention_weights_layer2\", axis=2)\n",
    "    focus_layer1 = layers.Dot((1, 1), name=\"focus_layer1\")\n",
    "    focus_layer2 = layers.Dot((2, 1), name=\"focus_layer2\")\n",
    "    pair_layer1 = layers.Concatenate(axis=-1, name=\"pair_layer1\")\n",
    "    pair_layer2 = layers.Concatenate(axis=-1, name=\"pair_layer2\")\n",
    "    \n",
    "    pairs = list()\n",
    "    \n",
    "    pairs += list(coatten_compare(\n",
    "        feature_concat_layer, coatten_layer,\n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        x1[0], x1[1], x2[0]))\n",
    "    \n",
    "    pairs += list(coatten_compare(\n",
    "        feature_concat_layer, coatten_layer,\n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        x1[0], x1[2], x2[1]))\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(pairs)\n",
    "    x = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"pair_dropout_layer\"))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(mlp_dim, name=\"pair_feature_map_layer\", activation=\"relu\"))(x)\n",
    "    x = layers.Flatten(name=\"pair_feature_flatten_layer1\")(x)\n",
    "    \n",
    "#     pooled_outputs = []\n",
    "#     for i in range(len(filter_sizes)):\n",
    "#         conv = layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu')(x)\n",
    "#         if pooling == 'max':\n",
    "#             conv = layers.GlobalMaxPooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         else:\n",
    "#             conv = layers.GlobalAveragePooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         pooled_outputs.append(conv)\n",
    "#     if len(pooled_outputs) > 1:\n",
    "#         x = layers.Concatenate(name='concated_layer')(pooled_outputs)\n",
    "#     else:\n",
    "#         x = conv\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'RemappedCoAttentionWeight': RemappedCoAttentionWeight,\n",
    "                       \"FeatureNormalization\": FeatureNormalization}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "histories = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "embed_dim = 384\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "model_dim = 10 \n",
    "mlp_dim = 60\n",
    "mlp_depth=1\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co_mlp = build_mlp_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    model_dim, mlp_dim, mlp_depth, drop_out, return_customized_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1_0 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_1 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_2 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 11, 384)      0           input1_0[0][0]                   \n",
      "                                                                 input1_1[0][0]                   \n",
      "                                                                 input1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 11, 10)       3850        time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_1[1][0]         \n",
      "                                                                 time_distributed_1[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input2_0 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2_1 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_flatten_layer1 (Flatten (None, 110)          0           time_distributed_2[0][0]         \n",
      "                                                                 time_distributed_2[1][0]         \n",
      "                                                                 time_distributed_2[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer2 (Dense)      (None, 10)           460         input2_0[0][0]                   \n",
      "                                                                 input2_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concate_layer (Concatenate)     (None, 350)          0           feature_flatten_layer1[0][0]     \n",
      "                                                                 feature_flatten_layer1[1][0]     \n",
      "                                                                 feature_flatten_layer1[2][0]     \n",
      "                                                                 feature_map_layer2[0][0]         \n",
      "                                                                 feature_map_layer2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_layer (BatchNormaliz (None, 350)          1400        concate_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 350)          0           batch_norm_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            1053        dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,763\n",
      "Trainable params: 6,063\n",
      "Non-trainable params: 700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.3792 - sparse_categorical_accuracy: 0.4870 - val_loss: 0.9332 - val_sparse_categorical_accuracy: 0.5925\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93317, saving model to best_mlp_model.hdf5\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 1s 455us/step - loss: 1.0352 - sparse_categorical_accuracy: 0.5790 - val_loss: 0.8406 - val_sparse_categorical_accuracy: 0.6123\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93317 to 0.84057, saving model to best_mlp_model.hdf5\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.8648 - sparse_categorical_accuracy: 0.6275 - val_loss: 0.7888 - val_sparse_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.84057 to 0.78884, saving model to best_mlp_model.hdf5\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.8111 - sparse_categorical_accuracy: 0.6470 - val_loss: 0.7795 - val_sparse_categorical_accuracy: 0.6498\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78884 to 0.77946, saving model to best_mlp_model.hdf5\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.7878 - sparse_categorical_accuracy: 0.6555 - val_loss: 0.7735 - val_sparse_categorical_accuracy: 0.6586\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.77946 to 0.77352, saving model to best_mlp_model.hdf5\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.7618 - sparse_categorical_accuracy: 0.6695 - val_loss: 0.7686 - val_sparse_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.77352 to 0.76860, saving model to best_mlp_model.hdf5\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 1s 440us/step - loss: 0.7496 - sparse_categorical_accuracy: 0.6745 - val_loss: 0.7651 - val_sparse_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76860 to 0.76512, saving model to best_mlp_model.hdf5\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 1s 443us/step - loss: 0.7304 - sparse_categorical_accuracy: 0.6795 - val_loss: 0.7460 - val_sparse_categorical_accuracy: 0.6586\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.76512 to 0.74600, saving model to best_mlp_model.hdf5\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.7140 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.7446 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.74600 to 0.74461, saving model to best_mlp_model.hdf5\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 1s 433us/step - loss: 0.7110 - sparse_categorical_accuracy: 0.6930 - val_loss: 0.7444 - val_sparse_categorical_accuracy: 0.6784\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.74461 to 0.74439, saving model to best_mlp_model.hdf5\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.7119 - sparse_categorical_accuracy: 0.6820 - val_loss: 0.7447 - val_sparse_categorical_accuracy: 0.6718\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.74439\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.6921 - sparse_categorical_accuracy: 0.6935 - val_loss: 0.7389 - val_sparse_categorical_accuracy: 0.6828\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.74439 to 0.73886, saving model to best_mlp_model.hdf5\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.6998 - sparse_categorical_accuracy: 0.6960 - val_loss: 0.7425 - val_sparse_categorical_accuracy: 0.6762\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.73886\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 1s 443us/step - loss: 0.6794 - sparse_categorical_accuracy: 0.7105 - val_loss: 0.7402 - val_sparse_categorical_accuracy: 0.6696\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.73886\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 1s 452us/step - loss: 0.6917 - sparse_categorical_accuracy: 0.7015 - val_loss: 0.7357 - val_sparse_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.73886 to 0.73566, saving model to best_mlp_model.hdf5\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.6949 - sparse_categorical_accuracy: 0.6910 - val_loss: 0.7326 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.73566 to 0.73265, saving model to best_mlp_model.hdf5\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.6821 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.6718\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.73265\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.6655 - sparse_categorical_accuracy: 0.7190 - val_loss: 0.7327 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73265\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.6575 - sparse_categorical_accuracy: 0.7100 - val_loss: 0.7392 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_mlp_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3)\n",
    "history = model.fit(X_train, y_tra, batch_size=20, epochs=20, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop],verbose=0)\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1_0 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_1 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_2 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 11, 384)      0           input1_0[0][0]                   \n",
      "                                                                 input1_1[0][0]                   \n",
      "                                                                 input1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cc_layer10 (Conv1D)             (None, 9, 10)        11530       time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_4[1][0]         \n",
      "                                                                 time_distributed_4[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cc_layer11 (Conv1D)             (None, 7, 10)        19210       time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_4[1][0]         \n",
      "                                                                 time_distributed_4[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_pooling_layer10 (GlobalM (None, 10)           0           cc_layer10[0][0]                 \n",
      "                                                                 cc_layer10[1][0]                 \n",
      "                                                                 cc_layer10[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_pooling_layer11 (GlobalM (None, 10)           0           cc_layer11[0][0]                 \n",
      "                                                                 cc_layer11[1][0]                 \n",
      "                                                                 cc_layer11[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input2_0 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2_1 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concated_layer (Concatenate)    (None, 20)           0           global_pooling_layer10[0][0]     \n",
      "                                                                 global_pooling_layer11[0][0]     \n",
      "                                                                 global_pooling_layer10[1][0]     \n",
      "                                                                 global_pooling_layer11[1][0]     \n",
      "                                                                 global_pooling_layer10[2][0]     \n",
      "                                                                 global_pooling_layer11[2][0]     \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer2 (Dense)      (None, 10)           460         input2_0[0][0]                   \n",
      "                                                                 input2_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concate_layer (Concatenate)     (None, 80)           0           concated_layer[0][0]             \n",
      "                                                                 concated_layer[1][0]             \n",
      "                                                                 concated_layer[2][0]             \n",
      "                                                                 feature_map_layer2[0][0]         \n",
      "                                                                 feature_map_layer2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_layer (BatchNormaliz (None, 80)           320         concate_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 80)           0           batch_norm_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            243         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,763\n",
      "Trainable params: 31,603\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "embed_dim = 384\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "model_dim = 10 \n",
    "filter_sizes = [3, 5]\n",
    "num_filters = [model_dim] * len(filter_sizes)\n",
    "mlp_dim = 60\n",
    "mlp_depth=1\n",
    "pooling='max'\n",
    "padding='valid'\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co_mccnn = build_multi_channel_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, model_dim, mlp_dim, mlp_depth, drop_out, pooling, padding, return_customized_layers\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93127, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93127 to 0.80869, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80869 to 0.80334, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80334 to 0.78452, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.78452\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.78452 to 0.77562, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.77562 to 0.76996, saving model to best_mc_cnn_model.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.76996\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76996\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.76996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_mc_cnn_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3)\n",
    "history = model.fit(X_train, y_tra, batch_size=20, epochs=20, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop],verbose=0)\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input2_0 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2_1 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer2 (Dense)      (None, 384)          17664       input2_0[0][0]                   \n",
      "                                                                 input2_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input1_0 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_1 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_layer (Reshape)         (None, 1, 384)       0           feature_map_layer2[0][0]         \n",
      "                                                                 feature_map_layer2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input1_2 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concate_pair1_layer (Concatenat (None, 23, 384)      0           input1_0[0][0]                   \n",
      "                                                                 input1_1[0][0]                   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concate_pair2_layer (Concatenat (None, 23, 384)      0           input1_0[0][0]                   \n",
      "                                                                 input1_2[0][0]                   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "coattention_weights_layer (Rema (None, 23, 23)       3850        concate_pair1_layer[0][0]        \n",
      "                                                                 concate_pair2_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "normalized_coattention_weights_ (None, 23, 23)       0           coattention_weights_layer[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normalized_coattention_weights_ (None, 23, 23)       0           coattention_weights_layer[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer2 (Dot)              (None, 23, 384)      0           normalized_coattention_weights_la\n",
      "                                                                 concate_pair2_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer1 (Dot)              (None, 23, 384)      0           normalized_coattention_weights_la\n",
      "                                                                 concate_pair1_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pair_layer1 (Concatenate)       (None, 23, 768)      0           concate_pair1_layer[0][0]        \n",
      "                                                                 focus_layer2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pair_layer2 (Concatenate)       (None, 23, 768)      0           concate_pair2_layer[0][0]        \n",
      "                                                                 focus_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concate_layer (Concatenate)     (None, 46, 768)      0           pair_layer1[0][0]                \n",
      "                                                                 pair_layer2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 46, 768)      0           concate_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 46, 5)        3845        time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pair_feature_flatten_layer1 (Fl (None, 230)          0           time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_layer (BatchNormaliz (None, 230)          920         pair_feature_flatten_layer1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 230)          0           batch_norm_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            693         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,972\n",
      "Trainable params: 26,512\n",
      "Non-trainable params: 460\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "embed_dim = 384\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "atten_dim = 10\n",
    "model_dim = 10\n",
    "filter_sizes = [1]\n",
    "num_filters = [20] * len(filter_sizes)\n",
    "mlp_dim = 5\n",
    "mlp_depth=1\n",
    "pooling='max'\n",
    "padding='valid'\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co_cacnn = build_inter_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, mlp_depth, drop_out, pooling, padding, return_customized_layers\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96359, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.96359 to 0.94849, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94849 to 0.86849, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86849 to 0.83940, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.83940 to 0.80570, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80570 to 0.80536, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.80536 to 0.78346, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.78346 to 0.77856, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.77856 to 0.76935, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.76935 to 0.76905, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76905\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76905\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.76905 to 0.75299, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75299\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.75299\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.75299\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.75299\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_intra_coatt_cnn_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop],verbose=0)\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input2_0 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2_1 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer2 (Dense)      (None, 384)          17664       input2_0[0][0]                   \n",
      "                                                                 input2_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input1_1 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_layer (Reshape)         (None, 1, 384)       0           feature_map_layer2[0][0]         \n",
      "                                                                 feature_map_layer2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input1_0 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1_2 (InputLayer)           (None, 11, 384)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concate_pair_layer (Concatenate (None, 12, 384)      0           input1_1[0][0]                   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "                                                                 input1_0[0][0]                   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "                                                                 input1_2[0][0]                   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "                                                                 input1_0[0][0]                   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "coattention_weights_layer (Rema (None, 12, 12)       3850        concate_pair_layer[0][0]         \n",
      "                                                                 concate_pair_layer[1][0]         \n",
      "                                                                 concate_pair_layer[2][0]         \n",
      "                                                                 concate_pair_layer[3][0]         \n",
      "__________________________________________________________________________________________________\n",
      "normalized_coattention_weights_ (None, 12, 12)       0           coattention_weights_layer[0][0]  \n",
      "                                                                 coattention_weights_layer[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normalized_coattention_weights_ (None, 12, 12)       0           coattention_weights_layer[0][0]  \n",
      "                                                                 coattention_weights_layer[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer2 (Dot)              (None, 12, 384)      0           normalized_coattention_weights_la\n",
      "                                                                 concate_pair_layer[1][0]         \n",
      "                                                                 normalized_coattention_weights_la\n",
      "                                                                 concate_pair_layer[3][0]         \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer1 (Dot)              (None, 12, 384)      0           normalized_coattention_weights_la\n",
      "                                                                 concate_pair_layer[0][0]         \n",
      "                                                                 normalized_coattention_weights_la\n",
      "                                                                 concate_pair_layer[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pair_layer1 (Concatenate)       (None, 12, 768)      0           concate_pair_layer[0][0]         \n",
      "                                                                 focus_layer2[0][0]               \n",
      "                                                                 concate_pair_layer[2][0]         \n",
      "                                                                 focus_layer2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pair_layer2 (Concatenate)       (None, 12, 768)      0           concate_pair_layer[1][0]         \n",
      "                                                                 focus_layer1[0][0]               \n",
      "                                                                 concate_pair_layer[3][0]         \n",
      "                                                                 focus_layer1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concate_layer (Concatenate)     (None, 48, 768)      0           pair_layer1[0][0]                \n",
      "                                                                 pair_layer2[0][0]                \n",
      "                                                                 pair_layer1[1][0]                \n",
      "                                                                 pair_layer2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (None, 48, 768)      0           concate_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 48, 5)        3845        time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pair_feature_flatten_layer1 (Fl (None, 240)          0           time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_layer (BatchNormaliz (None, 240)          960         pair_feature_flatten_layer1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 240)          0           batch_norm_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            723         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 27,042\n",
      "Trainable params: 26,562\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "embed_dim = 384\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "atten_dim = 10\n",
    "model_dim = 10\n",
    "filter_sizes = [1]\n",
    "num_filters = [20] * len(filter_sizes)\n",
    "mlp_dim = 5\n",
    "mlp_depth=1\n",
    "pooling='max'\n",
    "padding='valid'\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, intra_co_cacnn = build_intra_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, mlp_depth, drop_out, pooling, padding, return_customized_layers\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95775, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95775 to 0.89027, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.89027 to 0.83495, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.83495 to 0.78637, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.78637 to 0.77302, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.77302 to 0.75619, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.75619\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.75619 to 0.74454, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.74454 to 0.72763, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.72763\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.72763 to 0.71447, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.71447\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.71447\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.71447\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.71447\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.71447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_intra_coatt_cnn_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop],verbose=0)\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model: best_intra_coatt_cnn_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    \"best_mlp_model.hdf5\",\n",
    "    \"best_mc_cnn_model.hdf5\",\n",
    "    \"best_coatt_cnn_model.hdf5\",\n",
    "    \"best_intra_coatt_cnn_model.hdf5\"\n",
    "]\n",
    "\n",
    "cls_ =[\n",
    "    co_mlp, co_mccnn, co_cacnn, intra_co_cacnn\n",
    "]\n",
    "\n",
    "print(\"load best model: \" + str(model_paths[np.argmin(histories)]))\n",
    "model = models.load_model(\n",
    "    model_paths[np.argmin(histories)], cls_[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 326us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.265258</td>\n",
       "      <td>0.709774</td>\n",
       "      <td>0.024969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.958357</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.012646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.439234</td>\n",
       "      <td>0.363821</td>\n",
       "      <td>0.196945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.151126</td>\n",
       "      <td>0.556022</td>\n",
       "      <td>0.292852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.163635</td>\n",
       "      <td>0.780023</td>\n",
       "      <td>0.056342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.265258  0.709774  0.024969\n",
       "1  development-2  0.958357  0.028997  0.012646\n",
       "2  development-3  0.439234  0.363821  0.196945\n",
       "3  development-4  0.151126  0.556022  0.292852\n",
       "4  development-5  0.163635  0.780023  0.056342"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "sub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
